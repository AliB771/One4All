# ๐ข One4All: Modular Expert System with SLMs

**One4All** is a modular AI system that combines multiple **Specialized Language Models (SLMs)** trained on specific domains using **QLoRA**. Instead of relying on a single large LLM, One4All keeps a lightweight base LLM always loaded and dynamically loads LoRA weights for domain-specific experts. This design enables high performance with minimal hardware requirements.

---

## ๐ How It Works (English)

1. **Question Routing**  
   When a question is asked, it first goes to the **Router**, a lightweight classifier model. The router analyzes the query and determines which domain-specific expert(s) should handle it.

2. **Expert Processing**  
   Experts are **SLMs trained on specialized datasets** (e.g., medical, technology, e-commerce). Once the router identifies the relevant domain, the question is forwarded to the corresponding expert. Each expert loads only its LoRA weights on the base model, minimizing memory usage.

3. **Why SLMs?**  
   - SLMs allow running domain-specific tasks efficiently on smaller hardware (e.g., a single GPU with 8โ12GB VRAM).  
   - They reduce training and inference costs compared to a large monolithic LLM.  
   - Each expert can be independently updated without retraining the base model.

4. **Memory Efficiency**  
   Example:  
   - A base model (e.g., ParsBERT) with QLoRA LoRA weights for 3 experts may require **~12โ16GB RAM per expert** loaded sequentially.  
   - A comparable LLM (7B+ parameters) may require **>40GB RAM** to run a single instance.  
   Using SLMs + LoRA reduces hardware requirements and enables modular scaling.

5. **Example Flow**
```text
Question: "Which smartphone has the best camera in 2025?"
  โ
Router: Classifies as "Mobile/Technology"
  โ
Expert: Mobile Expert (SLM trained on mobile reviews)
  โ
Answer returned from expert
```

---

## ๐ Recommended SLMs for One4All

| Model Name           | Languages Supported | Domain Specialization       | QLoRA Memory Req |
|---------------------|------------------|----------------------------|----------------|
| ParsBERT            | Persian           | General / NLP tasks        | 4โ6 GB         |
| mBERT               | Multi-language    | General                    | 6โ8 GB         |
| CamemBERT            | French            | General                    | 4โ6 GB         |
| Qwen3-SLM           | English, Multi    | Causal Language Modeling   | 8โ12 GB        |
| SaadiBERT            | Persian           | Medical / Healthcare       | 4โ6 GB         |

> One4All can dynamically load multiple SLM experts depending on the task.

---

## ๐ง Technical Notes

- **Base model** always loaded in memory.  
- **LoRA adapters** applied for each expert separately.  
- Router is a **lightweight classification model** that routes queries efficiently.  
- Supports **multi-domain modular updates** without retraining the base LLM.  

**References:**  
- Hu et al., *LoRA: Low-Rank Adaptation of Large Language Models*, 2021  
- Qwen3 model papers and SLM studies in Persian and English NLP  

---

# ๐ข One4All: ุณุณุชู ูุชุฎุตุต ูุงฺููุงุฑ ุจุง SLMูุง

**One4All** ฺฉ ุณุณุชู ููุด ูุตููุน ูุงฺููุงุฑ ุงุณุช ฺฉู ุชุฑฺฉุจ ุงุฒ **ูุฏูโูุง ุฒุจุงู ุชุฎุตุต (SLM)** ุฑุง ุจุง ุงุณุชูุงุฏู ุงุฒ **QLoRA** ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ. ุจู ุฌุง ุงุณุชูุงุฏู ุงุฒ ฺฉ ูุฏู ุฒุจุงู ุจุฒุฑฺฏุ One4All ููุดู ูุฏู ูพุงู ุณุจฺฉ ุฑุง ููุฏ ูฺฏู ูโุฏุงุฑุฏ ู ููุท ูุฒูโูุง LoRA ูุฑุจูุท ุจู ูุชุฎุตุตโูุง ุฑุง ุจุงุฑฺฏุฐุงุฑ ูโฺฉูุฏ.

---

## ๐ ูุญูู ุนููฺฉุฑุฏ (ูุงุฑุณ)

1. **ูุณุฑุฏู ูพุฑุณุดโูุง**  
   ููุช ุณูุงู ูพุฑุณุฏู ูโุดูุฏุ ุงุจุชุฏุง ุจู **Router** ูโุฑูุฏ. ุงู ูุฏู ุณุจฺฉุ ุฒูููโ ุณูุงู ุฑุง ุดูุงุณุง ูโฺฉูุฏ ู ูุดุฎุต ูโฺฉูุฏ ฺฉุฏุงู ูุชุฎุตุต ุง ูุชุฎุตุตโูุง ุจุงุฏ ุจู ุขู ูพุงุณุฎ ุฏููุฏ.

2. **ูพุฑุฏุงุฒุด ุชูุณุท ูุชุฎุตุตโูุง**  
   ูุชุฎุตุตโูุง **SLMูุง ุขููุฒุด ุฏุฏู ุฏุฑ ุญูุฒูโูุง ุฎุงุต** ูุณุชูุฏ (ูุซูุงู ูพุฒุดฺฉุ ุชฺฉููููฺุ ุฎุฑุฏ). ูพุณ ุงุฒ ุดูุงุณุง ุญูุฒู ุชูุณุท Routerุ ุณูุงู ุจู ูุชุฎุตุต ูุฑุจูุทู ุงุฑุณุงู ูโุดูุฏ ู ุชููุง ูุฒูโูุง LoRA ุขู ูุชุฎุตุต ุฑู ูุฏู ูพุงู ุจุงุฑฺฏุฐุงุฑ ูโุดููุฏ ุชุง ุญุงูุธู ุจููู ุดูุฏ.

3. **ฺุฑุง SLMุ**  
   - SLMูุง ุงุฌุฑุง ฺฉุงุฑูุง ุชุฎุตุต ุฑุง ุจุง ุณุฎุชโุงูุฒุงุฑ ฺฉู (ูุซูุงู ฺฉ GPU ุจุง 8โ12GB VRAM) ุงูฺฉุงูโูพุฐุฑ ูโฺฉููุฏ.  
   - ูุฒููโูุง ุขููุฒุด ู ุงุณุชูุชุงุฌ ูุณุจุช ุจู LLMูุง ุจุฒุฑฺฏ ฺฉูุชุฑ ุงุณุช.  
   - ูุฑ ูุชุฎุตุต ุจูโุตูุฑุช ูุณุชูู ูุงุจู ุขูพุฏุช ุงุณุช ุจุฏูู ูุงุฒ ุจู ุขููุฒุด ูุฌุฏุฏ ูุฏู ูพุงู.

4. **ฺฉุงุฑุง ุญุงูุธู**  
   ูุซุงู:  
   - ูุฏู ูพุงู (ParsBERT) ุจุง ูุฒูโูุง LoRA ุณู ูุชุฎุตุต: **~12โ16GB RAM** ุจุฑุง ูุฑ ูุชุฎุตุต ุจูโุตูุฑุช ูุชูุงู ฺฉุงู ุงุณุช.  
   - ฺฉ LLM ุจุฒุฑฺฏ (7B+ ูพุงุฑุงูุชุฑ) ูุงุฒููุฏ **>40GB RAM** ุจุฑุง ฺฉ ููููู ุงุณุช.  
   ุจูุงุจุฑุงูุ SLMูุง + LoRA ูุงุฒ ุณุฎุชโุงูุฒุงุฑ ุฑุง ฺฉุงูุด ุฏุงุฏู ู ููุงุณโูพุฐุฑ ุฑุง ุขุณุงู ูโฺฉููุฏ.

5. **ูุซุงู ุฌุฑุงู ูพุฑุณุด**
```text
ุณูุงู: "ุจูุชุฑู ุฏูุฑุจู ฺฏูุด ุฏุฑ 2025 ฺฉุฏุงู ุงุณุชุ"
  โ
Router: ุฏุณุชูโุจูุฏ "ููุจุงู / ุชฺฉููููฺ"
  โ
Expert: ูุชุฎุตุต ููุจุงู (SLM ุขููุฒุด ุฏุฏู ุฑู ุจุฑุฑุณโูุง ููุจุงู)
  โ
ูพุงุณุฎ ุงุฒ ูุชุฎุตุต ุจุฑฺฏุดุช ุฏุงุฏู ูโุดูุฏ
```

---

## ๐ SLMูุง ุชูุตู ุดุฏู ุจุฑุง One4All

| ูุงู ูุฏู             | ุฒุจุงูโูุง ูพุดุชุจุงู ุดุฏู | ุญูุฒู ุชุฎุตุต                  | ูุงุฒ ุญุงูุธู QLoRA |
|--------------------|--------------------|-----------------------------|----------------|
| ParsBERT            | ูุงุฑุณ               | ุนููู / NLP                | 4โ6 GB         |
| mBERT               | ฺูุฏุฒุจุงู             | ุนููู                       | 6โ8 GB         |
| CamemBERT           | ูุฑุงูุณู              | ุนููู                       | 4โ6 GB         |
| Qwen3-SLM           | ุงูฺฏูุณุ ฺูุฏุฒุจุงู    | ูุฏูโุณุงุฒ ุฒุจุงู ุณุจุจ (Causal) | 8โ12 GB        |
| SaadiBERT           | ูุงุฑุณ               | ูพุฒุดฺฉ / ุณูุงูุช               | 4โ6 GB         |

> One4All ุจู ุตูุฑุช ุฏุงูุงูฺฉ ฺูุฏ ูุชุฎุตุต SLM ุฑุง ุจุณุชู ุจู ูพุฑุณุด ุจุงุฑฺฏุฐุงุฑ ูโฺฉูุฏ.

---

## ๐ง ูฺฉุงุช ูู

- **ูุฏู ูพุงู** ููุดู ุฏุฑ ุญุงูุธู ูฺฏู ุฏุงุดุชู ูโุดูุฏ.  
- **LoRA adapters** ุจุฑุง ูุฑ ูุชุฎุตุต ุจูโุตูุฑุช ุฌุฏุงฺฏุงูู ุงุนูุงู ูโุดููุฏ.  
- Router ูุฏู ุณุจฺฉ ู ฺฉุงุฑุขูุฏ ุงุณุช ฺฉู ูพุฑุณุดโูุง ุฑุง ูุณุฑุฏู ูโฺฉูุฏ.  
- ูพุดุชุจุงู ุงุฒ **ุขูพุฏุช ูุงฺููุงุฑ ฺูุฏ ุญูุฒูโุง** ุจุฏูู ูุงุฒ ุจู ุขููุฒุด ูุฌุฏุฏ ูุฏู ูพุงู.  

**ูุฑุงุฌุน:**  
- Hu ู ููฺฉุงุฑุงูุ *LoRA: Low-Rank Adaptation of Large Language Models*, 2021  
- ููุงูุงุช Qwen3 ู ูพฺููุดโูุง SLM ุฏุฑ ุญูุฒู NLP ูุงุฑุณ ู ุงูฺฏูุณ  

---

โ **Status:** ูพุฑูฺู ุฏุฑ ุญุงู ุขูพููุฏ ู ุงูุฌุงู ุชุณุชโูุง ููุง ุงุณุช.

